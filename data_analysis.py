import os
import re
from collections import Counter
import numpy as np

def clean_tweet(tweet):
    """
    æ¸…ç†æ¨æ–‡æ–‡æœ¬çš„å‡½æ•°
    
    è¾“å…¥:
        tweet: str, åŸå§‹æ¨æ–‡æ–‡æœ¬
        
    è¾“å‡º:
        str: æ¸…ç†åçš„æ–‡æœ¬
        
    åŠŸèƒ½:
        ç§»é™¤URLã€@ç”¨æˆ·åç­‰æ— å…³å†…å®¹
    """
    tweet = re.sub(r'http\S+|www\S+|https\S+', '', tweet, flags=re.MULTILINE)
    tweet = re.sub(r'@\w+', '', tweet)
    tweet = re.sub(r'^RT[\s]+', '', tweet)
    tweet = re.sub(r'\s+', ' ', tweet).strip()
    return tweet

def get_sentiment(text):
    """
    ç®€å•æƒ…æ„Ÿåˆ†æå‡½æ•°
    
    è¾“å…¥:
        text: str, è¾“å…¥æ–‡æœ¬
        
    è¾“å‡º:
        str: æƒ…æ„Ÿæ ‡ç­¾('positive'/'negative'/'neutral')
        
    åŠŸèƒ½:
        åŸºäºå…³é”®è¯çš„ç®€å•æƒ…æ„Ÿåˆ†æ
    """
    positive_words = ['happy', 'great', 'good', 'nice', 'love', 'ğŸ˜Š', 'ğŸ˜„', 'awesome', 'excellent', 'wonderful', 'best', 'amazing']
    negative_words = ['sad', 'bad', 'hate', 'terrible', 'awful', 'ğŸ˜¢', 'ğŸ˜ ', 'worst', 'horrible', 'poor', 'disappointed']
    
    text = text.lower()
    pos_count = sum(1 for word in positive_words if word in text)
    neg_count = sum(1 for word in negative_words if word in text)
    
    if pos_count > neg_count:
        return 'positive'
    elif neg_count > pos_count:
        return 'negative'
    return 'neutral'

def analyze_data(data_dir='data'):
    """
    æ•°æ®åˆ†æå‡½æ•°
    
    è¾“å…¥:
        data_dir: str, æ•°æ®ç›®å½•è·¯å¾„
        
    è¾“å‡º:
        None
        
    åŠŸèƒ½:
        åˆ†ææ•°æ®é›†çš„æ–‡æœ¬å’Œå›¾åƒç‰¹å¾,ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    """
    # è·å–æ‰€æœ‰æ–‡ä»¶
    files = os.listdir(data_dir)
    txt_files = sorted([f for f in files if f.endswith('.txt')])
    jpg_files = sorted([f for f in files if f.endswith('.jpg')])
    
    # åŸºæœ¬ç»Ÿè®¡
    print("\n=== åŸºæœ¬ç»Ÿè®¡ ===")
    print(f"æ–‡æœ¬æ–‡ä»¶æ€»æ•°: {len(txt_files)}")
    print(f"å›¾åƒæ–‡ä»¶æ€»æ•°: {len(jpg_files)}")
    
    # æ–‡æœ¬åˆ†æ
    text_lengths = []
    word_counts = []
    all_words = []
    sentiments = []
    emoji_count = 0
    url_count = 0
    mention_count = 0
    retweet_count = 0
    
    for txt_file in txt_files:
        with open(os.path.join(data_dir, txt_file), 'r', encoding='utf-8', errors='ignore') as f:
            text = f.read().strip()
            
            # ç»Ÿè®¡ç‰¹æ®Šå…ƒç´ 
            emoji_count += len(re.findall(r'[\U0001F300-\U0001F9FF]', text))
            url_count += len(re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text))
            mention_count += len(re.findall(r'@\w+', text))
            if text.startswith('RT '):
                retweet_count += 1
            
            cleaned_text = clean_tweet(text)
            
            # æƒ…æ„Ÿåˆ†æ
            sentiment = get_sentiment(cleaned_text)
            sentiments.append(sentiment)
            
            # ç»Ÿè®¡æ–‡æœ¬é•¿åº¦
            text_lengths.append(len(cleaned_text))
            
            # ç»Ÿè®¡è¯æ•°
            words = cleaned_text.split()
            word_counts.append(len(words))
            all_words.extend(words)
    
    # æ‰“å°ç»Ÿè®¡ç»“æœ
    print("\n=== æ–‡æœ¬ç‰¹å¾ç»Ÿè®¡ ===")
    print(f"Emojiæ•°é‡: {emoji_count}")
    print(f"URLé“¾æ¥æ•°é‡: {url_count}")
    print(f"@æåŠæ•°é‡: {mention_count}")
    print(f"è½¬å‘æ•°é‡: {retweet_count}")
    
    print("\n=== æ–‡æœ¬é•¿åº¦ç»Ÿè®¡ ===")
    print(f"å¹³å‡æ–‡æœ¬é•¿åº¦: {np.mean(text_lengths):.2f} å­—ç¬¦")
    print(f"æœ€çŸ­æ–‡æœ¬é•¿åº¦: {min(text_lengths)} å­—ç¬¦")
    print(f"æœ€é•¿æ–‡æœ¬é•¿åº¦: {max(text_lengths)} å­—ç¬¦")
    print(f"æ–‡æœ¬é•¿åº¦æ ‡å‡†å·®: {np.std(text_lengths):.2f}")
    
    print("\n=== è¯æ•°ç»Ÿè®¡ ===")
    print(f"å¹³å‡è¯æ•°: {np.mean(word_counts):.2f}")
    print(f"æœ€å°‘è¯æ•°: {min(word_counts)}")
    print(f"æœ€å¤šè¯æ•°: {max(word_counts)}")
    print(f"è¯æ•°æ ‡å‡†å·®: {np.std(word_counts):.2f}")
    
    print("\n=== æƒ…æ„Ÿåˆ†å¸ƒ ===")
    sentiment_counter = Counter(sentiments)
    total = len(sentiments)
    for sentiment, count in sentiment_counter.items():
        percentage = (count / total) * 100
        print(f"{sentiment}: {count} ({percentage:.2f}%)")
    
    print("\n=== é«˜é¢‘è¯ç»Ÿè®¡ï¼ˆtop 20ï¼‰===")
    word_freq = Counter(all_words)
    for word, count in word_freq.most_common(20):
        print(f"{word}: {count}")
    
    # å›¾åƒæ–‡ä»¶å¤§å°ç»Ÿè®¡
    image_sizes = []
    for jpg_file in jpg_files:
        size = os.path.getsize(os.path.join(data_dir, jpg_file)) / 1024  # è½¬æ¢ä¸ºKB
        image_sizes.append(size)
    
    print("\n=== å›¾åƒç»Ÿè®¡ ===")
    print(f"å¹³å‡å›¾åƒå¤§å°: {np.mean(image_sizes):.2f} KB")
    print(f"æœ€å°å›¾åƒå¤§å°: {min(image_sizes):.2f} KB")
    print(f"æœ€å¤§å›¾åƒå¤§å°: {max(image_sizes):.2f} KB")
    print(f"å›¾åƒå¤§å°æ ‡å‡†å·®: {np.std(image_sizes):.2f} KB")
    
    # æ‰“å°ä¸€äº›ç¤ºä¾‹
    print("\n=== æ–‡æœ¬ç¤ºä¾‹ ===")
    for i in range(min(5, len(txt_files))):
        with open(os.path.join(data_dir, txt_files[i]), 'r', encoding='utf-8', errors='ignore') as f:
            text = f.read().strip()
            cleaned = clean_tweet(text)
            sentiment = get_sentiment(cleaned)
            print(f"\nç¤ºä¾‹ {i+1}:")
            print(f"åŸå§‹æ–‡æœ¬: {text}")
            print(f"æ¸…ç†åæ–‡æœ¬: {cleaned}")
            print(f"æƒ…æ„Ÿæ ‡ç­¾: {sentiment}")

if __name__ == '__main__':
    analyze_data() 